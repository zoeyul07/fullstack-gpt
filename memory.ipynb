{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature=0.1, \n",
    "  streaming=True,\n",
    "  callbacks=[\n",
    "    StreamingStdOutCallbackHandler()\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'), AIMessage(content='how are you')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#랭체인에는 5갖 정도의 메모리가 있는데, 모드 저장 방식도 다르고, 각자만의 장단점이 있다.\n",
    "#챗복에 메모리를 추가하지 않으면 챗봇은 아무것도 기억할 수 없고, 대화를 이해할 수 있는 능력이 없다.\n",
    "#openai 에서 default 로 제공하는 것은 메모리를 지원하지 않아 stateless 하다.\n",
    "#모델에게 무언가를 보내면 응답한 후 모든 대화 내용을 저장하지 않아 잊는다.\n",
    "#chatGPT에는 메모리가 탑재되어있어 실제로 어떤 사람과 이야기하고 있다는 느낌을 들게 한다.\n",
    "\n",
    "#Conversation Buffer memory - 단순히 이전 대화 내용 전체를 저장하는 메모리\n",
    "#단점은 대화 내용이 길어질수록 메모리도 계속 커지기 때문에 비효율적이다.\n",
    "#모델 자체에는 메모리가 없기 때문에 모델에 요청을 보낼 때 이전 대화 기록 전체를 같이 보내야한다.\n",
    "#대화가 길어질수록 메모리에 보내야될 대화가 길어진다.\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#text completion을 할때 유용하다.예측을하고, 텍스트를 자동 완성 하고 싶을 때\n",
    "#챗모델과 작업을 하려면 스트링이 아닌 ai 메세지와 human메세지가 모두 필요하다.\n",
    "#메모리 종류와 무관하게 api는 다 같다.(return_message, save_context, load_memory...등 을 갖고 있음)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"how are you\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#대화의 특정 부분만을 저장하는 메모리\n",
    "#가장 최근의 5개의 메세지를 저장한다고 했을 떄, 6번째 메세지가 추가되었을 때 가장 오래된 메세지는 버려지는 방식\n",
    "#메모리를 특정 크기로 유지할 수 있는것이 장점\n",
    "#단잠은 챗봇이 전체 대화가 아닌 최근 대화에만 집중한다.\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#k-how many interaction you want to remember\n",
    "memory = ConversationBufferWindowMemory(\n",
    "  return_messages=True, \n",
    "  k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(5,5)\n",
    "\n",
    "# 첫번째는 사라짐\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#메세지를 오는 그대로 저장하는것이 아니라 요약해서 저장한다.\n",
    "#conversationBuffferMemory를 사용하면 대화가 진행될수록 저장된 모든 메세지가 많이지면서 연결된다.\n",
    "#초반에는 이전보다 더 많은 토큰과 저장공간을 차지하게 되지만, summary를 사용하면 대화가 쌓일수록 요약되어 토큰의 양도 줄어들고 훨씬 나은 방법이 된다.\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "#메모리를 실행하는데 비용이 듬\n",
    "memory= ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"hi I'm seoyul, I live in south korea\", \"Wow that is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Seoyul from South Korea expresses admiration for their country. The AI responds by expressing a desire to visit South Korea.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"south korea is so pretty\", \"I wish i could go!!!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation summary memory + conversation buffer memory의 결합\n",
    "#메모리에 보내온 베세지의 수를 저장\n",
    "#Limit 에 도달하는 순간 이전에 일어났던 대화들을 잊어버리는 것이 아닌 요약하게된다.\n",
    "#가장 최근의 상호작용을 계속 추적한다.\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ConversationSummaryBufferMemory(\n",
    "  llm = llm,\n",
    "  #메세지가 요약되기 전 가능한 토큰 수의 최대값\n",
    "  max_token_limit=10,\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "add_message(\"south korea is so pretty\", \"I wish i could go!!!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
