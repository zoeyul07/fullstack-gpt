{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature=0.1, \n",
    "  streaming=True,\n",
    "  callbacks=[\n",
    "    StreamingStdOutCallbackHandler()\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'), AIMessage(content='how are you')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#랭체인에는 5갖 정도의 메모리가 있는데, 모드 저장 방식도 다르고, 각자만의 장단점이 있다.\n",
    "#챗복에 메모리를 추가하지 않으면 챗봇은 아무것도 기억할 수 없고, 대화를 이해할 수 있는 능력이 없다.\n",
    "#openai 에서 default 로 제공하는 것은 메모리를 지원하지 않아 stateless 하다.\n",
    "#모델에게 무언가를 보내면 응답한 후 모든 대화 내용을 저장하지 않아 잊는다.\n",
    "#chatGPT에는 메모리가 탑재되어있어 실제로 어떤 사람과 이야기하고 있다는 느낌을 들게 한다.\n",
    "\n",
    "#Conversation Buffer memory - 단순히 이전 대화 내용 전체를 저장하는 메모리\n",
    "#단점은 대화 내용이 길어질수록 메모리도 계속 커지기 때문에 비효율적이다.\n",
    "#모델 자체에는 메모리가 없기 때문에 모델에 요청을 보낼 때 이전 대화 기록 전체를 같이 보내야한다.\n",
    "#대화가 길어질수록 메모리에 보내야될 대화가 길어진다.\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#text completion을 할때 유용하다.예측을하고, 텍스트를 자동 완성 하고 싶을 때\n",
    "#챗모델과 작업을 하려면 스트링이 아닌 ai 메세지와 human메세지가 모두 필요하다.\n",
    "#메모리 종류와 무관하게 api는 다 같다.(return_message, save_context, load_memory...등 을 갖고 있음)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"how are you\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#대화의 특정 부분만을 저장하는 메모리\n",
    "#가장 최근의 5개의 메세지를 저장한다고 했을 떄, 6번째 메세지가 추가되었을 때 가장 오래된 메세지는 버려지는 방식\n",
    "#메모리를 특정 크기로 유지할 수 있는것이 장점\n",
    "#단잠은 챗봇이 전체 대화가 아닌 최근 대화에만 집중한다.\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#k-how many interaction you want to remember\n",
    "memory = ConversationBufferWindowMemory(\n",
    "  return_messages=True, \n",
    "  k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(5,5)\n",
    "\n",
    "# 첫번째는 사라짐\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#메세지를 오는 그대로 저장하는것이 아니라 요약해서 저장한다.\n",
    "#conversationBuffferMemory를 사용하면 대화가 진행될수록 저장된 모든 메세지가 많이지면서 연결된다.\n",
    "#초반에는 이전보다 더 많은 토큰과 저장공간을 차지하게 되지만, summary를 사용하면 대화가 쌓일수록 요약되어 토큰의 양도 줄어들고 훨씬 나은 방법이 된다.\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "#메모리를 실행하는데 비용이 듬\n",
    "memory= ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"hi I'm seoyul, I live in south korea\", \"Wow that is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Seoyul from South Korea expresses admiration for their country. The AI responds by expressing a desire to visit South Korea.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"south korea is so pretty\", \"I wish i could go!!!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation summary memory + conversation buffer memory의 결합\n",
    "#메모리에 보내온 베세지의 수를 저장\n",
    "#Limit 에 도달하는 순간 이전에 일어났던 대화들을 잊어버리는 것이 아닌 요약하게된다.\n",
    "#가장 최근의 상호작용을 계속 추적한다.\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "llm = ConversationSummaryBufferMemory(\n",
    "  llm = llm,\n",
    "  #메세지가 요약되기 전 가능한 토큰 수의 최대값\n",
    "  max_token_limit=10,\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "add_message(\"south korea is so pretty\", \"I wish i could go!!!\")\n",
    "\n",
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대화중의 엔티티의 knowledge graph를 만든다.\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "  llm=llm,\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "add_message(\"Hi I'm Zoe, I live in south korea\", \"wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Zoe: Zoe lives in South Korea.')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#요약하지만 대화에서 entity를 뽑아낸다\n",
    "memory.load_memory_variables({\"input\":\"who is Zoe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Zoe: Zoe lives in South Korea. Zoe likes coffee.')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"Zoe likes coffee\", \"wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\":\"What does Zoe like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  You are a helpful AI talking to a human.\n",
      "  \n",
      "  \n",
      "  Human:My name is Zoe\n",
      "  You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Zoe! How can I assist you today?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llm chain - off the shelf(일반적인 목적을 가진) chain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  #템플릿 안에 컨텐츠를 넣으라는 의미로 템플릿 안에 메모리가 히스토리를 저장하도록 한 곳을 적어줌\n",
    "  #메모리는 템플릿 안에 대화 기록을 저장하는 공간을 찾고, ㅏㅈ동으로 메모리의 기록을 넣는다.\n",
    "  memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "  You are a helpful AI talking to a human.\n",
    "  \n",
    "  {chat_history}\n",
    "  Human:{question}\n",
    "  You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  prompt=PromptTemplate.from_template(template),\n",
    "  #chain을 실행했을 때 프롬프트 로그들을 확인할 수 있다.\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Zoe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  You are a helpful AI talking to a human.\n",
      "  \n",
      "  Human: My name is Zoe\n",
      "AI: Hello Zoe! How can I assist you today?\n",
      "  Human:I live in seoul\n",
      "  You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know! Is there anything specific you would like assistance with regarding Seoul?\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  You are a helpful AI talking to a human.\n",
      "  \n",
      "  Human: My name is Zoe\n",
      "AI: Hello Zoe! How can I assist you today?\n",
      "Human: I live in seoul\n",
      "AI: That's great to know! Is there anything specific you would like assistance with regarding Seoul?\n",
      "  Human:What is my name?\n",
      "  You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Zoe.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")\n",
    "\n",
    "# 우리가 원하는 방식으로 프롬프트에게 대화 기록을 추가해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Zoe\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Zoe! How can I assist you today?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문자열형태, message 형태\n",
    "#메모리 옵션에 return_message=True 를 사용하면 다음 명령어 사용시 메세지 형태로 리턴된다.\n",
    "#memory.load_memory_variables({})\n",
    "from langchain.prompts import MessagesPlaceholder,ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  #누가 보냈는지 알 수 없는, 예측하기 어려운 메세지의 양과 제한 없는 양의 메세지를 가질 수 있다.\n",
    "  # 다음을 사용하면 메세지가 얼마나 누구에게로부터 왔는지 모르지만 memory class로 대체될것이다.\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory():\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "#RunnablePassthrough 를 사용하면 load_memory 함수의 결과값을 chat_history input으로 넣어주라고 알려줄 수 있다.\n",
    "#프롬프트가 format 됮기 전에 함수를 실행시키는걸 허락해준다. 원하는 변수를 넣을 수 있고, 그 변수는 prompt에 주어지게 된다.\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "#chain을 invoke 할때마다 chat history를 더해줘야함\n",
    "chain.invoke({\n",
    "  \"chat_history\": load_memory(),\n",
    "  \"question\":\"My name is Seoyul\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder,ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke(\n",
    "    {\n",
    "      \"question\":question\n",
    "    }\n",
    "  )\n",
    "  #메모리를 수동으로 관리하고 있기 때문에 사용자와 기계간의 인터렉션을 메모리에 저장해야한다.\n",
    "  #save_context - input과 output을 메모리에 저장한다.\n",
    "  memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "  print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Seoyul! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is seoyul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Seoyul.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
